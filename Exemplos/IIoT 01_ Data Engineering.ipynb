{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34a0af72-2e56-4aa0-a8c2-6809ee754a65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.widgets.text(\"Database\", \"\")\n",
    "# dbutils.widgets.text(\"Event Hub Name\",\"\")\n",
    "# dbutils.widgets.text(\"IoT Hub Connection String\", \"\")\n",
    "# dbutils.widgets.text(\"External Location\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0def383e-0905-4580-90e9-6d6952cff987",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# End to End Industrial IoT (IIoT) on Databricks\n",
    "## Part 1 - Data Engineering\n",
    "\n",
    "<img style=\"float: left; margin-right: 20px\" width=\"400px\" src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/manufacturing/lakehouse-iot-turbine/lakehouse-manuf-iot-1.png\" />\n",
    "\n",
    "\n",
    "<br/>\n",
    "<div style=\"padding-left: 420px\">\n",
    "Our first step is to ingest and clean the raw data we received so that our Data Analyst team can start running analysis on top of it.\n",
    "\n",
    "\n",
    "<img src=\"https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-logo.png\" style=\"float: right; margin-top: 20px\" width=\"200px\">\n",
    "\n",
    "### Delta Lake\n",
    "\n",
    "All the tables we'll create in the Lakehouse will be stored as Delta Lake tables. [Delta Lake](https://delta.io) is an open storage framework for reliability and performance. <br/>\n",
    "It provides many functionalities such as *(ACID Transaction, DELETE/UPDATE/MERGE, Clone zero copy, Change data Capture...)* <br />\n",
    "For more details on Delta Lake, run `dbdemos.install('delta-lake')`\n",
    "\n",
    "The notebook is broken into sections following these steps:\n",
    "1. **Data Ingestion** - stream real-time raw sensor data from Azure IoT Hubs into the Delta format in cloud storage\n",
    "2. **Data Processing** - stream process sensor data from raw (Bronze) to silver (aggregated) to gold (enriched) Delta tables on cloud storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "283371b8-cb76-4bd2-8314-95160d7cdea5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 0. Environment Setup\n",
    "\n",
    "The pre-requisites are listed below:\n",
    "\n",
    "### Services Required\n",
    "* Azure IoT Hub \n",
    "\n",
    "### Databricks Configuration Required\n",
    "* 3-node (min) Databricks Cluster running **DBR 11.3 ML** and the following libraries:\n",
    "  * **Azure Event Hubs Connector for Databricks** - Maven coordinates `com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.17`\n",
    "  * **Azure IoT Hub Device SDK** - PyPI coordinates `azure-iot-device==2.12.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5940075d-e4fc-4509-b053-61079f239cbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "DB = dbutils.widgets.get(\"Database\")\n",
    "ROOT_PATH = dbutils.widgets.get(\"External Location\")\n",
    "BRONZE_PATH = ROOT_PATH + \"bronze/\"\n",
    "SILVER_PATH = ROOT_PATH + \"silver/\"\n",
    "GOLD_PATH = ROOT_PATH + \"gold/\"\n",
    "CHECKPOINT_PATH = ROOT_PATH + \"checkpoints/\"\n",
    "\n",
    "# Other initializations\n",
    "# Optimally, use Databricks Secrets to store/retrieve sensitive information (ie. dbutils.secrets.get('iot','iot-hub-connection-string'))\n",
    "ehConf = { \n",
    "  'ehName':dbutils.widgets.get(\"Event Hub Name\"),\n",
    "  'eventhubs.connectionString':sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(dbutils.widgets.get(\"IoT Hub Connection String\")),\n",
    "  # If it's required to reprocess the whole history from IoT Hub\n",
    "  # 'eventhubs.startingPosition':'{\"offset\":\"-1\", \"seqNo\":-1, \"enqueuedTime\":null, \"isInclusive\":true}'\n",
    "}\n",
    "\n",
    "# Enable auto compaction and optimized writes in Delta\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 12)\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.streaming.stateStore.providerClass\", \"com.databricks.sql.streaming.state.RocksDBStateStoreProvider\")\n",
    "\n",
    "# Imports\n",
    "from pyspark.sql import functions as F\n",
    "import mlflow\n",
    "\n",
    "# Define default database\n",
    "spark.sql(f'USE {DB}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c68462d-c89f-4a03-b93e-96cb1d49ec44",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Data Ingestion from IoT Hub\n",
    "Databricks provides a native connector to IoT and Event Hubs. Below, we will use PySpark Structured Streaming to read from an IoT Hub stream of data and write the data in it's raw format directly into Delta. \n",
    "\n",
    "<img src=\"https://sguptasa.blob.core.windows.net/random/iiot_blog/iot_simulator.gif\" width=800>\n",
    "\n",
    "We have two separate types of data payloads in our IoT Hub:\n",
    "1. **Turbine Sensor readings** - this payload contains `date`,`timestamp`,`deviceid`,`rpm` and `angle` fields\n",
    "2. **Weather Sensor readings** - this payload contains `date`,`timestamp`,`temperature`,`humidity`,`windspeed`, and `winddirection` fields\n",
    "\n",
    "We split out the two payloads into separate streams and write them both into Delta locations on cloud Storage. We are able to query these two Bronze tables *immediately* as the data streams in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91c7933f-60de-4516-a467-c6ed2d28ef57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1a. Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4c6fc6d-6022-4d3b-afe5-43a64e806569",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Schema of incoming data from IoT hub\n",
    "schema = \"timestamp timestamp, deviceId string, temperature double, humidity double, windspeed double, winddirection string, rpm double, angle double, power double\"\n",
    "\n",
    "# Read directly from IoT Hub using the EventHubs library for Databricks\n",
    "iot_stream = (\n",
    "  spark.readStream.format(\"eventhubs\")\n",
    "    .options(**ehConf)\n",
    "    .load()\n",
    "    .withColumn('reading', F.from_json(F.col('body').cast('string'), schema))\n",
    "    .select('reading.*', F.to_date('reading.timestamp').alias('date'))\n",
    ")\n",
    "\n",
    "# Split our IoT Hub stream into separate streams and write them both into their own Delta locations\n",
    "# Write turbine events\n",
    "(iot_stream.filter('temperature is null')\n",
    "  .select('date','timestamp','deviceId','rpm','angle','power')\n",
    "  .writeStream.format('delta')\n",
    "  .partitionBy('date')\n",
    "  .option(\"checkpointLocation\", CHECKPOINT_PATH + \"turbine_raw\")\n",
    "  .toTable('turbine_raw')\n",
    ")\n",
    "\n",
    "# Write weather events\n",
    "(iot_stream.filter(iot_stream.temperature.isNotNull())\n",
    "  .select('date','deviceid','timestamp','temperature','humidity','windspeed','winddirection')\n",
    "  .writeStream.format('delta')\n",
    "  .partitionBy('date')\n",
    "  .option(\"checkpointLocation\", CHECKPOINT_PATH + \"weather_raw\")\n",
    "  .toTable('weather_raw')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc07dbc7-87f8-48ea-aa7e-50c7e6790c38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%sql \n-- We can query the data directly from storage immediately as soon as it starts streams into Delta \nSELECT * FROM weather_raw --WHERE deviceid = 'WindTurbine-1'",
       "commandTitle": "Line chart (Plotly)",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "plotlyLine": [
         {
          "key": "yRange",
          "value": ""
         },
         {
          "key": "showPoints",
          "value": false
         },
         {
          "key": "logScale",
          "value": false
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "plotlyLine",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "414",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "1fff3f5a-2dd0-4306-ad3d-b8cd17df52ba",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": "sum",
       "pivotColumns": [],
       "position": 1.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "1500",
       "workflows": null,
       "xColumns": [
        "timestamp"
       ],
       "yColumns": [
        "rpm"
       ]
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql \n",
    "-- We can query the data directly from storage immediately as soon as it starts streams into Delta \n",
    "SELECT * FROM turbine_raw WHERE deviceid = 'WindTurbine-1' AND `timestamp` > current_timestamp() - INTERVAL 2 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa253ccb-e4b4-4a63-8dfc-87ca109c81bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Data Processing\n",
    "While our raw sensor data is being streamed into Bronze Delta tables on cloud storage, we can create streaming pipelines on this data that flow it through Silver and Gold data sets.\n",
    "\n",
    "We will use the following schema for Silver and Gold data sets:\n",
    "\n",
    "<img src=\"https://sguptasa.blob.core.windows.net/random/iiot_blog/iot_delta_bronze_to_gold.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f159b3a5-1672-4ce8-9226-69020549e2fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2a. Silver Layer\n",
    "The first step of our processing pipeline will clean and aggregate the measurements to 1 hour intervals. \n",
    "\n",
    "Since we are aggregating time-series values and there is a likelihood of late-arriving data and data changes, we will use the [**MERGE**](https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/language-manual/merge-into?toc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Fazure-databricks%2Ftoc.json&bc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Fbread%2Ftoc.json) functionality of Delta to upsert records into target tables. \n",
    "\n",
    "MERGE allows us to upsert source records into a target storage location. This is useful when dealing with time-series data as:\n",
    "1. Data often arrives late and requires aggregation states to be updated\n",
    "2. Historical data needs to be backfilled while streaming data is feeding into the table\n",
    "\n",
    "When streaming source data, `foreachBatch()` can be used to perform a merges on micro-batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a65e06b-865c-45e4-b347-dc073162ce00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create functions to merge turbine and weather data into their target Delta tables\n",
    "def merge_delta(incremental, target): \n",
    "  incremental.dropDuplicates(['date','window','deviceid']).createOrReplaceTempView(\"incremental\")\n",
    "  \n",
    "  try:\n",
    "    # MERGE records into the target table using the specified join key\n",
    "    incremental._jdf.sparkSession().sql(f\"\"\"\n",
    "      MERGE INTO {target} t\n",
    "      USING incremental i\n",
    "      ON i.date=t.date AND i.window = t.window AND i.deviceId = t.deviceid\n",
    "      WHEN MATCHED THEN UPDATE SET *\n",
    "      WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "  except:\n",
    "    # If the â€ arget table does not exist, create one\n",
    "    incremental.writeTo(target).partitionedBy('date').createOrReplace()\n",
    "    \n",
    "# Stream turbine events to silver layer\n",
    "(spark.readStream.format('delta').table(\"turbine_raw\")\n",
    "  .filter(\"date = current_date()\")   # Only for demo purposes\n",
    "  .withWatermark('timestamp', '30 seconds')\n",
    "  .groupBy('deviceId','date',F.window('timestamp','15 seconds'))\n",
    "  .agg(F.avg('rpm').alias('rpm'), F.avg(\"angle\").alias(\"angle\"), F.avg(\"power\").alias(\"power\"))\n",
    "  .selectExpr('deviceId', 'date', 'window.start as window', 'rpm', 'angle', 'power')\n",
    "  .writeStream\n",
    "  .foreachBatch(lambda i, b: merge_delta(i, \"turbine_agg\"))\n",
    "  .outputMode(\"update\")\n",
    "  .option(\"checkpointLocation\", CHECKPOINT_PATH + \"turbine_agg\")\n",
    "  .start()\n",
    ")\n",
    "\n",
    "# Stream wheather events to silver layer\n",
    "(spark.readStream.format('delta').table(\"weather_raw\")\n",
    "  .filter(\"date = current_date()\")   # Only for demo purposes\n",
    "  .withWatermark('timestamp', '30 seconds')\n",
    "  .groupBy('deviceid','date',F.window('timestamp','15 seconds'))\n",
    "  .agg({\"temperature\":\"avg\",\"humidity\":\"avg\",\"windspeed\":\"avg\",\"winddirection\":\"last\"})\n",
    "  .selectExpr('date','window.start as window','deviceid','`avg(temperature)` as temperature','`avg(humidity)` as humidity',\n",
    "              '`avg(windspeed)` as windspeed','`last(winddirection)` as winddirection')\n",
    "  .writeStream\n",
    "  .foreachBatch(lambda i, b: merge_delta(i, \"weather_agg\"))\n",
    "  .outputMode(\"update\")\n",
    "  .option(\"checkpointLocation\", CHECKPOINT_PATH + \"weather_agg\")\n",
    "  .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f00423c-d48e-45d3-894c-1be2aa272945",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- As data gets merged in real-time to our hourly table, we can query it immediately\n",
    "SELECT * FROM turbine_agg t JOIN weather_agg w ON (t.date=w.date AND t.window=w.window) \n",
    "WHERE t.deviceid='WindTurbine-1' AND t.window > current_timestamp() - INTERVAL 10 minutes\n",
    "ORDER BY t.window DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "122c8aba-9fb2-46dc-b6f7-d91ba70aac60",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2b. Gold Layer & ML Inference\n",
    "Next we perform a streaming join of weather and turbine readings to create one enriched dataset we can use for data science and model training.\n",
    "\n",
    "Also, our Data Science team has already used this data to build <a href=\"https://e2-demo-field-eng.cloud.databricks.com/ml/models/VR%20IIoT%20-%20Life%20-%20WindTurbine-1?o=1444828305810485\">this predictive maintenance model</a> and saved it into MLflow Model Registry (we'll see how to do that next).\n",
    "\n",
    "One of the key value of the Lakehouse is that we can easily load this model and predict faulty turbines with into our pipeline directly.\n",
    "\n",
    "Note that we don't have to worry about the model framework (sklearn or other), MLflow abstract that for us.\n",
    "\n",
    "All we have to do is load the model, and call it as a Spark UDF (or SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dfc60aa-0371-471a-97e8-b4cd222427b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load our model to predict the turbine remaining life\n",
    "predict_remaining_life = mlflow.pyfunc.spark_udf(spark, 'models:/VR IIoT - Life - WindTurbine-1/production', result_type='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9bd8532-f6da-4493-8bcc-acbe3e162e61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read streams from both Silver Delta tables\n",
    "turbine_agg = (spark.readStream.format('delta')\n",
    "  .option(\"ignoreChanges\", True)\n",
    "  .table('turbine_agg')\n",
    "  .withWatermark('window', '30 seconds')\n",
    "  .filter(\"date = current_date()\")   # Only for demo purposes\n",
    ")\n",
    "weather_agg = (spark.readStream.format('delta')\n",
    "  .option(\"ignoreChanges\", True)\n",
    "  .table('weather_agg')\n",
    "  .withWatermark('window', '30 seconds')\n",
    "  .drop('deviceid')\n",
    "  .filter(\"date = current_date()\")   # Only for demo purposes\n",
    ")\n",
    "\n",
    "# Identify the last maintenance for each turbine\n",
    "dates_df = spark.sql('select deviceid, max(date) as maint_date from turbine_maintenance group by deviceid')\n",
    "\n",
    "# Join all streams\n",
    "turbine_enriched = (turbine_agg\n",
    "  .join(weather_agg, ['date','window'])\n",
    "  .join(dates_df, on='deviceid', how='left')\n",
    ")\n",
    "\n",
    "# Write the stream to a foreachBatch function which performs the MERGE as before\n",
    "(turbine_enriched\n",
    "  .selectExpr('date','deviceid','window','rpm','angle','power','temperature','humidity','windspeed','winddirection',\n",
    "              \"datediff(date,ifnull(maint_date,to_date('2022-11-21'))) as age\")\n",
    "  .withColumn('remaining_life', predict_remaining_life())\n",
    "  .writeStream\n",
    "  .foreachBatch(lambda i, b: merge_delta(i, \"turbine_enriched\"))\n",
    "  .option(\"checkpointLocation\", CHECKPOINT_PATH + \"turbine_enriched\")\n",
    "  .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f7282f0-a59e-4b41-9c08-4fbb9ea620e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%sql WITH q AS (SELECT * FROM turbine_enriched WHERE deviceid='WindTurbine-000001' AND window > current_timestamp() - INTERVAL 10 minutes) SELECT `window`,SUM(`rpm`) `column_7ef070073`,SUM(`temperature`) `column_7ef070075` FROM q GROUP BY `window`",
       "commandTitle": "RPM x Temperature",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "window",
             "id": "column_7ef070071"
            },
            "y": [
             {
              "column": "rpm",
              "id": "column_7ef070073",
              "transform": "SUM"
             },
             {
              "column": "temperature",
              "id": "column_7ef070075",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0[.]00000",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_7ef070073": {
             "type": "line",
             "yAxis": 0
            },
            "column_7ef070075": {
             "type": "line",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "2bce0fa6-99fe-47e2-bcd7-18e985d52789",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 18.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "window",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "window",
           "type": "column"
          },
          {
           "alias": "column_7ef070073",
           "args": [
            {
             "column": "rpm",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "alias": "column_7ef070075",
           "args": [
            {
             "column": "temperature",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%sql WITH q AS (SELECT * FROM turbine_enriched WHERE deviceid='WindTurbine-000001' AND window > current_timestamp() - INTERVAL 5 minutes) SELECT `window`,AVG(`remaining_life`) `column_5b228c21459` FROM q GROUP BY `window`",
       "commandTitle": "Remaining Life",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "deviceId",
             "id": "column_5b228c21553"
            },
            "x": {
             "column": "window",
             "id": "column_5b228c21490"
            },
            "y": [
             {
              "column": "remaining_life",
              "id": "column_5b228c21459",
              "transform": "AVG"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0[.]00000",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_5b228c21459": {
             "type": "line",
             "yAxis": 0
            },
            "remaining_life": {
             "type": "line",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "16dc41f1-0d58-4d35-82cf-c4d7fe07ad18",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 19.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "window",
           "type": "column"
          },
          {
           "column": "deviceId",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "window",
           "type": "column"
          },
          {
           "alias": "column_5b228c21459",
           "args": [
            {
             "column": "remaining_life",
             "type": "column"
            }
           ],
           "function": "AVG",
           "type": "function"
          },
          {
           "column": "deviceId",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql SELECT * FROM turbine_enriched WHERE deviceid IN ('WindTurbine-10','WindTurbine-20') AND window > current_timestamp() - INTERVAL 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a19555a-8493-4d22-8973-d616669932fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Generate Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d89434-3da4-4301-ac2c-0d32961bb636",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from resources.iiot_event_generator import iiot_event_generator\n",
    "iiot_event_generator(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4625da60-0b4d-4998-bfaa-f7b9ff414ab0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "Our pipeline is now ready. We have an end-to-end cycle and our ML model has been integrated seamlessly by our Data Engineering team.\n",
    "\n",
    "For more details on model training, open the [model training notebook]($../IIoT 02: Data Science and Machine Learning).\n",
    "\n",
    "Our final dataset includes our ML prediction for our Predictive Maintenance use-case. \n",
    "\n",
    "We are now ready to build our Predictive Maintenance dashboard to track the main KPIs and status of our entire Wind Turbine Farm in <a href=\"/sql/dashboards/f27ba14b-1be9-4dbf-944b-f40fbbb47aa5\">DBSQL</a> and/or <a href=\"https://vorodrigues.grafana.net/d/acef080c-3e23-4d11-8e1e-8109646c7c76/wind-turbines-monitoring\"> Grafana</a>."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1974893261343044,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "Database",
      "width": 176
     },
     {
      "breakBefore": false,
      "name": "External Location",
      "width": 176
     },
     {
      "breakBefore": false,
      "name": "Event Hub Name",
      "width": 176
     },
     {
      "breakBefore": false,
      "name": "IoT Hub Connection String",
      "width": 176
     }
    ]
   },
   "notebookName": "IIoT 01_ Data Engineering",
   "widgets": {
    "Database": {
     "currentValue": "vr_iiot.dev",
     "nuid": "1fb398a6-08bb-4d89-bd05-be37220beed3",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Database",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Event Hub Name": {
     "currentValue": "vr-iothub",
     "nuid": "ae192cab-e4a4-4447-a80c-9a0ae4ba97a7",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Event Hub Name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "External Location": {
     "currentValue": "s3://databricks-vr/iiot/",
     "nuid": "2027b33a-06ee-42b3-85c1-c482b728fabe",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "External Location",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "IoT Hub Connection String": {
     "currentValue": "Endpoint=sb://iothub-ns-vr-iothub-25443751-12653431af.servicebus.windows.net/;SharedAccessKeyName=iothubowner;SharedAccessKey=SlTJ9Kd9Bc3I+hNfMKpGx1/ixZPg/dD2jAIoTLRjgBE=;EntityPath=vr-iothub",
     "nuid": "8146210b-b93f-45cf-9b2b-1bc5a5c27925",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "IoT Hub Connection String",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "name": "IIoT End-to-End",
  "notebookId": 1335585283234210
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
