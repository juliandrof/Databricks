{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "934a2ff3-676e-4076-ab7b-8b0278e66cf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import Row\n",
    "import datetime\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"true\")\n",
    "# Sample data with date column\n",
    "data = [\n",
    "    (\"Alice\",\"Alice\", \"34\", datetime.date(2023, 10, 1)),\n",
    "    (\"Alice\",\"Bob\", \"45\", datetime.date(2023, 10, 1)),\n",
    "    (\"Alice\",\"Cathy\", \"29\", datetime.date(2024, 10, 1)),\n",
    "    (\"Alice\",\"David\", \"40\", datetime.date(2024, 10, 2)),\n",
    "    (\"Alice\",\"Eva\", \"35\", datetime.date(2023, 10, 2)),\n",
    "    (\"Alice\",\"Frank\", \"50\", datetime.date(2023, 10, 2)),\n",
    "    (\"Alice\",\"Grace\", \"30\", datetime.date(2024, 10, 3)),\n",
    "    (\"Alice\",\"Hank\", \"60\", datetime.date(2023, 10, 3)),\n",
    "    (\"Alice\",\"Ivy\", \"25\", datetime.date(2023, 10, 3)),\n",
    "    (\"Alice\",\"Jack\", \"55\", datetime.date(2023, 10, 4)),\n",
    "    (\"Alice\",\"Kathy\", \"28\", datetime.date(2023, 10, 4))\n",
    "]\n",
    "columns = [\"name\",\"Name\", \"Age\", \"Date\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Write DataFrame to S3 as Parquet, partitioned by the date column\n",
    "df.write.mode(\"overwrite\").partitionBy(\"Date\").parquet(\"/Volumes/jsf_catalog/ecommerce/parquets/teste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9543af90-47bc-4588-84b6-4d8212b2529f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_read = spark.read.option(\"recursiveFileLookup\", \"true\").parquet(\"/Volumes/jsf_catalog/ecommerce/parquets/teste\")\n",
    "display(df_read)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Criando e lendo Parquets",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
